import{_ as o}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as n,o as s,c as i,a as e,b as t,d as a}from"./app-iHBaMDR3.js";const l={},h=e("h2",{id:"mixtral",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mixtral","aria-hidden":"true"},"#"),t(" Mixtral")],-1),c=e("p",null,"Mistral AI开源采用SMoE架构的Mixtral 8x7B模型，整体效能超越Llama 2 70B与GPT-3.5",-1),p={href:"https://mistral.ai/news/mixtral-of-experts/",target:"_blank",rel:"noopener noreferrer"},x={href:"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1",target:"_blank",rel:"noopener noreferrer"},_=e("h2",{id:"moe-mixture-of-experts",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#moe-mixture-of-experts","aria-hidden":"true"},"#"),t(" MoE：Mixture-Of-Experts")],-1),d={href:"https://zhuanlan.zhihu.com/p/335024684",target:"_blank",rel:"noopener noreferrer"},m=e("h2",{id:"moe-pytorch实现",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#moe-pytorch实现","aria-hidden":"true"},"#"),t(" MoE PyTorch实现")],-1),u={href:"https://github.com/lucidrains/mixture-of-experts",target:"_blank",rel:"noopener noreferrer"};function f(g,b){const r=n("ExternalLinkIcon");return s(),i("div",null,[h,c,e("p",null,[t("官网： "),e("a",p,[t("https://mistral.ai/news/mixtral-of-experts/"),a(r)])]),e("p",null,[t("huggingface 网址： "),e("a",x,[t("https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"),a(r)])]),_,e("p",null,[e("a",d,[t("https://zhuanlan.zhihu.com/p/335024684"),a(r)])]),m,e("p",null,[e("a",u,[t("https://github.com/lucidrains/mixture-of-experts"),a(r)])])])}const B=o(l,[["render",f],["__file","mixtral.html.vue"]]);export{B as default};
