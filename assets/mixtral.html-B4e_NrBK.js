const t=JSON.parse('{"key":"v-7233a0b2","path":"/ai/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/mixtral.html","title":"Mixtral大模型","lang":"zh-CN","frontmatter":{"title":"Mixtral大模型","index":true,"order":1,"icon":"laptop-code","dir":{"collapsible":false},"description":"Mixtral Mistral AI开源采用SMoE架构的Mixtral 8x7B模型，整体效能超越Llama 2 70B与GPT-3.5 官网： https://mistral.ai/news/mixtral-of-experts/ huggingface 网址： https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1","head":[["meta",{"property":"og:url","content":"https://zhang555.github.io/ai/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/mixtral.html"}],["meta",{"property":"og:site_name","content":"程序员弓哥"}],["meta",{"property":"og:title","content":"Mixtral大模型"}],["meta",{"property":"og:description","content":"Mixtral Mistral AI开源采用SMoE架构的Mixtral 8x7B模型，整体效能超越Llama 2 70B与GPT-3.5 官网： https://mistral.ai/news/mixtral-of-experts/ huggingface 网址： https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-12-17T15:21:49.000Z"}],["meta",{"property":"article:author","content":"程序员弓哥"}],["meta",{"property":"article:modified_time","content":"2023-12-17T15:21:49.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Mixtral大模型\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2023-12-17T15:21:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"程序员弓哥\\",\\"url\\":\\"https://zhang555.github.io\\"}]}"]]},"headers":[{"level":2,"title":"Mixtral","slug":"mixtral","link":"#mixtral","children":[]},{"level":2,"title":"MoE：Mixture-Of-Experts","slug":"moe-mixture-of-experts","link":"#moe-mixture-of-experts","children":[]},{"level":2,"title":"MoE PyTorch实现","slug":"moe-pytorch实现","link":"#moe-pytorch实现","children":[]}],"git":{"createdTime":1702825706000,"updatedTime":1702826509000,"contributors":[{"name":"zhanggong","email":"zhanggong@xinyu668.com","commits":3}]},"readingTime":{"minutes":0.23,"words":70},"filePathRelative":"ai/大语言模型/mixtral.md","localizedDate":"2023年12月17日","excerpt":"<h2> Mixtral</h2>\\n<p>Mistral AI开源采用SMoE架构的Mixtral 8x7B模型，整体效能超越Llama 2 70B与GPT-3.5</p>\\n<p>官网：\\n<a href=\\"https://mistral.ai/news/mixtral-of-experts/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://mistral.ai/news/mixtral-of-experts/</a></p>\\n<p>huggingface 网址：\\n<a href=\\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1</a></p>","autoDesc":true}');export{t as data};
